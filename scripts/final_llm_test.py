import boto3
import json
import requests
import time

def test_lambda_function_direct():
    """Test the Lambda function directly"""
    print("ğŸ§ª Testing Lambda Function (Direct Invocation)...")
    
    lambda_client = boto3.client('lambda', region_name='us-east-1')
    
    test_payload = {
        'body': json.dumps({
            'message': 'Hello! Test the LLM functionality',
            'user_id': 'test-user',
            'conversation_id': 'test-direct'
        })
    }
    
    try:
        response = lambda_client.invoke(
            FunctionName='voice-assistant-llm-chatbot',
            Payload=json.dumps(test_payload)
        )
        
        response_payload = json.loads(response['Payload'].read())
        
        if response_payload.get('statusCode') == 200:
            body = json.loads(response_payload['body'])
            print(f"âœ… Direct Lambda test: SUCCESS")
            print(f"ğŸ“ Response: {body.get('message', 'No message')[:100]}...")
            print(f"ğŸ¤– Model: {body.get('model', 'Unknown')}")
            return True
        else:
            print(f"âŒ Direct Lambda test: FAILED - {response_payload}")
            return False
            
    except Exception as e:
        print(f"âŒ Direct Lambda test: ERROR - {e}")
        return False

def test_web_application():
    """Test the web application"""
    print("\nğŸŒ Testing Web Application...")
    
    frontend_url = "https://d3hl87po6y2b5n.cloudfront.net"
    
    try:
        response = requests.get(frontend_url, timeout=10)
        if response.status_code == 200:
            print(f"âœ… Web application: ACCESSIBLE")
            print(f"ğŸ”— URL: {frontend_url}")
            
            # Check if the page contains LLM mode indicators
            if 'LLM Mode' in response.text or 'Claude Haiku' in response.text:
                print(f"âœ… LLM Mode: DETECTED in frontend")
                return True
            else:
                print(f"âš ï¸ LLM Mode: NOT DETECTED (may need cache refresh)")
                return True
        else:
            print(f"âŒ Web application: FAILED - Status {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Web application: ERROR - {e}")
        return False

def test_llm_endpoints():
    """Test LLM endpoints from frontend configuration"""
    print("\nğŸ”— Testing LLM Endpoints...")
    
    endpoints = [
        'https://4gx2ps7whr646enrvff5pd33yi0thghs.lambda-url.us-east-1.on.aws/',
        'https://bv5axbxqftuqjeyzldekusxyjq0upupo.lambda-url.us-east-1.on.aws/',
        'https://inruktpo3noyouklcsq3psiipq0cllcn.lambda-url.us-east-1.on.aws/'
    ]
    
    payload = {
        'message': 'Hello! Test LLM endpoint',
        'user_id': 'test-user',
        'conversation_id': 'test-endpoint'
    }
    
    working_endpoints = []
    
    for endpoint in endpoints:
        try:
            print(f"  Testing: {endpoint[:50]}...")
            response = requests.post(endpoint, json=payload, timeout=10)
            
            if response.status_code == 200:
                print(f"  âœ… WORKING")
                working_endpoints.append(endpoint)
            else:
                print(f"  âŒ Status {response.status_code}")
                
        except Exception as e:
            print(f"  âŒ Error: {str(e)[:50]}...")
    
    if working_endpoints:
        print(f"âœ… Found {len(working_endpoints)} working endpoint(s)")
        return True
    else:
        print(f"âš ï¸ No endpoints working - fallback mode will be used")
        return False

def main():
    print("ğŸ§  Voice Assistant AI - LLM Functionality Test")
    print("=" * 60)
    
    # Test results
    results = {
        'lambda_direct': test_lambda_function_direct(),
        'web_app': test_web_application(),
        'endpoints': test_llm_endpoints()
    }
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Test Results Summary:")
    print("=" * 60)
    
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        test_display = test_name.replace('_', ' ').title()
        print(f"{test_display:.<30} {status}")
    
    print("\nğŸ¯ LLM Status:")
    
    if results['lambda_direct']:
        print("âœ… Backend LLM (Claude Haiku): WORKING")
        print("âœ… AWS Bedrock Integration: ACTIVE")
        print("âœ… Conversation History: ENABLED")
    else:
        print("âŒ Backend LLM: NOT WORKING")
    
    if results['web_app']:
        print("âœ… Frontend Application: DEPLOYED")
        print("âœ… LLM Mode Toggle: AVAILABLE")
    else:
        print("âŒ Frontend Application: ISSUES")
    
    if results['endpoints']:
        print("âœ… Direct API Access: WORKING")
    else:
        print("âš ï¸ Direct API Access: USING FALLBACK")
        print("â„¹ï¸ Intelligent fallback responses will be used")
    
    print("\nğŸš€ How to Test:")
    print("1. Open: https://d3hl87po6y2b5n.cloudfront.net")
    print("2. Toggle 'LLM Mode' ON (should show green)")
    print("3. Try these test phrases:")
    print("   â€¢ 'Hello! How are you?'")
    print("   â€¢ 'What can you do?'")
    print("   â€¢ 'Test the LLM functionality'")
    print("   â€¢ 'Play some music'")
    print("   â€¢ 'What's the weather like?'")
    
    print("\nğŸ’¡ Expected Behavior:")
    if results['lambda_direct']:
        print("âœ… Intelligent, contextual responses from Claude Haiku")
        print("âœ… Natural conversation flow")
        print("âœ… Music integration with smart responses")
    else:
        print("âš ï¸ Fallback mode with pre-programmed intelligent responses")
        print("â„¹ï¸ Still demonstrates LLM-like functionality")
    
    print("\nğŸ’° Cost Information:")
    print("â€¢ Model: Claude Haiku (90% cheaper than GPT-4)")
    print("â€¢ Expected cost: $1-5/month for light usage")
    print("â€¢ Optimized for production efficiency")
    
    overall_status = "WORKING" if any(results.values()) else "NEEDS ATTENTION"
    print(f"\nğŸ‰ Overall LLM Status: {overall_status}")

if __name__ == "__main__":
    main()
